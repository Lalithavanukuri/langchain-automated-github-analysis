{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOukuxh34CLBjfXQ+8A4T86",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lalithavanukuri/langchain-automated-github-analysis/blob/main/Langchain_git_automated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztrNSO1EyDfh",
        "outputId": "b594bddc-59bb-43e2-887a-1a0c72b23e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Github automated analysis\n"
          ]
        }
      ],
      "source": [
        "print(\"Github automated analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask nbformat requests gitpython torch transformers\n",
        "import os\n",
        "import shutil\n",
        "import nbformat\n",
        "import requests\n",
        "import git\n",
        "import torch\n",
        "from nbformat import read, write, validate\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Fetch user repositories from GitHub\n",
        "def fetch_user_repositories(github_url):\n",
        "    # Extract the username from the user URL\n",
        "    username = github_url.split('/')[-1]\n",
        "\n",
        "    # GitHub API endpoint to retrieve user repositories\n",
        "    api_url = f'https://api.github.com/users/{username}/repos'\n",
        "\n",
        "    # Make an API request to retrieve repository data\n",
        "    response = requests.get(api_url)\n",
        "    repos = response.json()\n",
        "\n",
        "    # Iterate through the repositories\n",
        "    for repo in repos:\n",
        "        clone_url = repo['clone_url']\n",
        "        repo_name = repo['name']\n",
        "        repo_directory = os.path.join(\"./temp\", repo_name)\n",
        "\n",
        "        # Check if the repository directory already exists\n",
        "        if os.path.exists(repo_directory):\n",
        "            print(f\"Updating repository: {repo_name}\")\n",
        "            repo = git.Repo(repo_directory)\n",
        "\n",
        "            # Pull the latest changes from the remote repository\n",
        "            origin = repo.remote(name='origin')\n",
        "            origin.pull()\n",
        "\n",
        "        else:\n",
        "            print(f\"Cloning repository: {repo_name}\")\n",
        "            # Clone the repository if it doesn't exist locally\n",
        "            git.Repo.clone_from(clone_url, repo_directory)\n",
        "\n",
        "    # Return the list of repository names\n",
        "    return [repo['name'] for repo in repos]\n",
        "\n",
        "\n",
        "# Preprocess code in repositories\n",
        "def preprocess_code(repository):\n",
        "    # Define a temporary directory to store preprocessed files\n",
        "    temp_directory = \"./temp\"\n",
        "    os.makedirs(temp_directory, exist_ok=True)\n",
        "\n",
        "    # Clone the repository locally if it doesn't already exist\n",
        "    repo_directory = os.path.join(temp_directory, repository)\n",
        "    if not os.path.exists(repo_directory):\n",
        "        clone_command = f\"git clone https://github.com/{repository}.git {repo_directory}\"\n",
        "        os.system(clone_command)\n",
        "\n",
        "    # Iterate through the repository files\n",
        "    for root, dirs, files in os.walk(repo_directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_extension = os.path.splitext(file)[1]\n",
        "\n",
        "            # Preprocess specific file types\n",
        "            if file_extension == \".ipynb\":\n",
        "                preprocess_jupyter_notebook(file_path)\n",
        "            elif file_extension == \".py\":\n",
        "                preprocess_python_file(file_path)\n",
        "\n",
        "    # Remove temporary directory\n",
        "    shutil.rmtree(temp_directory)\n",
        "\n",
        "\n",
        "# Preprocess Jupyter notebook file\n",
        "def preprocess_jupyter_notebook(file_path):\n",
        "    # Read the Jupyter notebook\n",
        "    with open(file_path, 'r') as file:\n",
        "        notebook = read(file, as_version=4)\n",
        "\n",
        "    # Normalize the notebook to add the missing ID field\n",
        "    validate(notebook)\n",
        "\n",
        "    # Remove outputs from all cells\n",
        "    for cell in notebook.cells:\n",
        "        if 'outputs' in cell:\n",
        "            cell['outputs'] = []\n",
        "\n",
        "    # Save the preprocessed Jupyter notebook\n",
        "    with open(file_path, 'w') as file:\n",
        "        write(notebook, file, version=4)\n",
        "\n",
        "\n",
        "# Preprocess Python file\n",
        "def preprocess_python_file(file_path):\n",
        "    # Read the Python file\n",
        "    with open(file_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Remove comments and whitespace\n",
        "    lines = [line for line in lines if not line.strip().startswith(\"#\")]\n",
        "\n",
        "    # Save the preprocessed Python file\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.writelines(lines)\n",
        "\n",
        "\n",
        "# Assess technical complexity using GPT\n",
        "def assess_technical_complexity(code):\n",
        "    try:\n",
        "        # Define your own GPT2 model and tokenizer or load a pre-trained model\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # Apply prompt engineering techniques to evaluate the code's technical complexity\n",
        "        prompt = \"The code provided is:\"\n",
        "        input_text = f\"{prompt} {code}\"\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "        # Set attention mask and pad token ID\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=pad_token_id,\n",
        "            max_length=100,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Compute the complexity score based on generated text (e.g., using LangChain)\n",
        "        complexity_score = compute_complexity_score(generated_text)\n",
        "\n",
        "        return complexity_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing technical complexity: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Compute complexity score using LangChain or other techniques\n",
        "def compute_complexity_score(text):\n",
        "    # Placeholder implementation\n",
        "    # You can replace this with your own implementation using LangChain or other techniques\n",
        "\n",
        "    # Calculate complexity score based on the provided text\n",
        "    complexity_score = len(text)  # Placeholder complexity score calculation\n",
        "\n",
        "    return complexity_score\n",
        "\n",
        "\n",
        "# Identify most technically complex repository\n",
        "def identify_most_complex_repository(repositories):\n",
        "    most_complex_repository = None\n",
        "    max_complexity_score = 0\n",
        "\n",
        "    for repository in repositories:\n",
        "        preprocess_code(repository)\n",
        "\n",
        "        # Assess the technical complexity of the code in the repository\n",
        "        code = get_repository_code(repository)\n",
        "        complexity_score = assess_technical_complexity(code)\n",
        "\n",
        "        if complexity_score is not None and complexity_score > max_complexity_score:\n",
        "            max_complexity_score = complexity_score\n",
        "            most_complex_repository = repository\n",
        "\n",
        "    return most_complex_repository\n",
        "\n",
        "\n",
        "# Get the code from the repository\n",
        "def get_repository_code(repository):\n",
        "    # Define the directory where the repository is cloned\n",
        "    repo_directory = f\"./temp/{repository}\"\n",
        "\n",
        "    # Iterate through the repository files\n",
        "    code = \"\"\n",
        "    for root, dirs, files in os.walk(repo_directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_extension = os.path.splitext(file)[1]\n",
        "\n",
        "            # Read and append code from specific file types\n",
        "            if file_extension == \".ipynb\" or file_extension == \".py\":\n",
        "                with open(file_path, \"r\") as file:\n",
        "                    code += file.read()\n",
        "\n",
        "    return code\n",
        "\n",
        "\n",
        "# Generate justification using GPT\n",
        "def generate_justification(repository):\n",
        "    try:\n",
        "        # Define your own GPT2 model and tokenizer or load a pre-trained model\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # Craft prompts based on the assessment and the selected repository\n",
        "        prompt = f\"The repository {repository} was selected as the most technically complex because:\"\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Set attention mask and pad token ID\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=pad_token_id,\n",
        "            max_length=200,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        justification = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return justification\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating justification: {e}\")\n",
        "        return None\n",
        "\n",
        "def first():\n",
        "    user_url = input(\"Enter GitHub user URL: \")\n",
        "    repositories = fetch_user_repositories(user_url)\n",
        "    most_complex_repository = identify_most_complex_repository(repositories)\n",
        "\n",
        "    if most_complex_repository is not None:\n",
        "        justification = generate_justification(most_complex_repository)\n",
        "\n",
        "        print(\"Most Complex Repository:\", most_complex_repository)\n",
        "        print(\"Justification:\", justification)\n",
        "    else:\n",
        "        print(\"No repositories found or error occurred.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t861g1rFyMMS",
        "outputId": "b2a70dce-0563-4561-8488-afdf7ce04614"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.17.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.3.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat) (3.7.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Enter GitHub user URL: https://github.com/Lohitharepalle\n",
            "Cloning repository: Earthquake-Prediction-using-Gradient-Boosting\n",
            "Cloning repository: Frontend\n",
            "Cloning repository: Frontend-technologies\n",
            "Cloning repository: githubworkshop-1\n",
            "Cloning repository: JokesExtension\n",
            "Cloning repository: Lohitharepalle\n",
            "Cloning repository: Machine-Learning\n",
            "Cloning repository: My-Blog-Editor\n",
            "Cloning repository: My-Portfolio\n",
            "Cloning repository: taskrepo\n",
            "Cloning repository: Weather-Visualisation-Using-Augmented-Reality\n",
            "Most Complex Repository: Earthquake-Prediction-using-Gradient-Boosting\n",
            "Justification: The repository Earthquake-Prediction-using-Gradient-Boosting was selected as the most technically complex because:\n",
            "\n",
            "It's hard to know how to get a reliable estimate of the likelihood of a earthquake. The probability of an earthquake will depend on the location of your earthquake detection system.\n",
            " (The probability that you could have a good earthquake, even if you have no earthquake detectors, is less than 3%).\n",
            " and the probability, if there is one, that it is a likely one. If there are no earthquakes, you can't tell by looking at the data that the magnitude of any one of those earthquakes is zero. (It is usually the case that there might be two earthquakes and one earthquake detector, but that's unlikely.)\n",
            ", and if no one is working on it, no individual person can tell what the earthquake would be like. There are thousands of possible earthquake sensors. It's very hard for the average person to be certain about one or more. This means,\n"
          ]
        }
      ]
    }
  ]
}